## Education
Business Administration Finance ( Sept 2018 - Sept 2021 )

## Work Experience

**Data Analyst @ Grof (_March 2024 - Dec 2024_)**
*Identified and resolved issue in ETL pipelines
- Improving data accuracy for over 50% of revenue-related metrics, ensuring reliable insights for decision-making.
*Optimized SQL queries for MRR analysis,
- Incorporating add-ons, one-time charges, and first-time fee, resulting in a 20% reduction in report generation time and enhanced accuracy of recurring revenue forecasts.
*Developed advanced churn and growth rate models
- Accounting for complex scenarios like paused, canceled, and resumed subscriptions, which improved subscription mrr amount accuracy to 100% and supported strategic retention initiatives.
* Automated the generation of KPI reports
- Integrating data from multiple platforms using SQL queries and Python ETL scripts. Consolidated data into a unified dashboard, eliminating the need for manual extraction from disparate platforms and reducing report preparation time by 80%. This enabled management to access up-to-date KPIs in real-time, supporting faster and more informed decision-making.

## Projects
### Merchandise Sales Analysis: Driving Insights for Growth
In this project, I analyzed a merchandise sales dataset to uncover patterns, optimize sales strategies, and improve customer satisfaction. The goal was to build a robust data pipeline and create dynamic visualizations to assist stakeholders in making data-driven decisions.

#### Step 1: Data Extraction
The first step involved gathering data from multiple platforms. In a real-world scenario, this data would come from APIs or platform exports. For this project, the data was extracted from a CSV file.

![image](https://github.com/user-attachments/assets/f6b077ed-4a7e-4a69-ba5c-fdc5898e15b1)

#### Step 2: Data Cleaning
After extraction, I checked for missing values and inconsistent entries. For example, some columns had missing customer ages or improperly formatted sales amounts. Cleaning ensured the dataset was accurate and analysis-ready.

![image](https://github.com/user-attachments/assets/37551173-d6b4-4a07-9374-4da52d1b3653)

#### Step 3: Data Transformation
The dataset required transformations to prepare it for deeper analysis. This included formatting dates and restructuring sales amounts. SQL was used for this purpose, as it provides an efficient way to handle large datasets.

![image](https://github.com/user-attachments/assets/487023a5-09e6-48b1-bc94-8ca8de4b1050)

Examples of other subscriptions where government charges or non-recurring charges are excluded when calculating revenue.
![image](https://github.com/user-attachments/assets/7e899bc0-17f2-42df-bd0c-99d899d0137c)

#### Step 4: Load to Database
With the cleaned and transformed data, I loaded it into a cloud-based database (e.g., BigQuery) to enable scalability and efficient querying.

![image](https://github.com/user-attachments/assets/77fdcf53-4f02-4c57-86ea-a02b7f4a181e)

#### Step 5: Dashboard Creation
Finally, I visualized the insights using Superset / Metabase / Lookerstudio. The dashboards displayed key metrics, such as top-performing product categories, customer demographics, and shipping efficiency.

![image](https://github.com/user-attachments/assets/0420144a-7ea3-4fc9-86fc-96093814022c)

![image](https://github.com/user-attachments/assets/a60360c6-0574-4165-9248-3534669a56b9)

![image](https://github.com/user-attachments/assets/5c33c70a-284e-4819-b7a2-20fd2000ec43)

![image](https://github.com/user-attachments/assets/bb23f39c-204f-4e40-87de-73c4b5774de4)
